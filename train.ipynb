{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "055e6878",
   "metadata": {},
   "source": [
    "### This notebook reproduces:\n",
    "\n",
    "[Variationally Regularized Graph-based Representation Learning for Electronic Health Records](https://arxiv.org/abs/1912.03761).\n",
    "\n",
    "The code in this notebook is adapted from the original code included with the paper.\n",
    "\n",
    "[Paper Code](https://github.com/NYUMedML/GNN_for_EHR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81b360cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from torch.utils.data import Dataset\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fce3921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/rtikes/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\r\n",
      "Tue Apr 25 07:53:09 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:02:00.0  On |                  N/A |\r\n",
      "|  0%   41C    P8    23W / 170W |    167MiB / 12288MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1800      G   /usr/lib/xorg/Xorg                 76MiB |\r\n",
      "|    0   N/A  N/A      1960      G   /usr/bin/gnome-shell               68MiB |\r\n",
      "|    0   N/A  N/A      4018      G   ...RendererForSitePerProcess       16MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# Verify Nvidia Drivers setup (optional uncomment if desired)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7638a4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39e49462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9396e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   5882 KiB |   5882 KiB |   5882 KiB |      0 B   |\n",
      "|       from large pool |   5296 KiB |   5296 KiB |   5296 KiB |      0 B   |\n",
      "|       from small pool |    585 KiB |    585 KiB |    585 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   5882 KiB |   5882 KiB |   5882 KiB |      0 B   |\n",
      "|       from large pool |   5296 KiB |   5296 KiB |   5296 KiB |      0 B   |\n",
      "|       from small pool |    585 KiB |    585 KiB |    585 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   5881 KiB |   5881 KiB |   5881 KiB |      0 B   |\n",
      "|       from large pool |   5296 KiB |   5296 KiB |   5296 KiB |      0 B   |\n",
      "|       from small pool |    585 KiB |    585 KiB |    585 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |\n",
      "|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |\n",
      "|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  16646 KiB |  17167 KiB |  17167 KiB | 534016 B   |\n",
      "|       from large pool |  15183 KiB |  15183 KiB |  15183 KiB |      0 B   |\n",
      "|       from small pool |   1462 KiB |   1984 KiB |   1984 KiB | 534016 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      25    |      25    |      25    |       0    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |      24    |      24    |      24    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      25    |      25    |      25    |       0    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |      24    |      24    |      24    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       2    |       2    |       2    |       0    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       2    |       2    |       2    |       0    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify available memory on GPU\n",
    "print(torch.cuda.memory_summary(device=0, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b01a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "def clone_params(param, N):\n",
    "    return nn.ParameterList([copy.deepcopy(param) for _ in range(N)])\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "\n",
    "class GraphLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, hidden_features, out_features, num_of_nodes,\n",
    "                 num_of_heads, dropout, alpha, concat=True):\n",
    "        super(GraphLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "        self.num_of_nodes = num_of_nodes\n",
    "        self.num_of_heads = num_of_heads\n",
    "        self.W = clones(nn.Linear(in_features, hidden_features), num_of_heads)\n",
    "        self.a = clone_params(nn.Parameter(torch.rand(size=(1, 2 * hidden_features)), requires_grad=True), num_of_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(out_features, out_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        if not concat:\n",
    "            self.V = nn.Linear(hidden_features, out_features)\n",
    "        else:\n",
    "            self.V = nn.Linear(num_of_heads * hidden_features, out_features)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        if concat:\n",
    "            self.norm = LayerNorm(hidden_features)\n",
    "        else:\n",
    "            self.norm = LayerNorm(hidden_features)\n",
    "\n",
    "    def initialize(self):\n",
    "        for i in range(len(self.W)):\n",
    "            nn.init.xavier_normal_(self.W[i].weight.data)\n",
    "        for i in range(len(self.a)):\n",
    "            nn.init.xavier_normal_(self.a[i].data)\n",
    "        if not self.concat:\n",
    "            nn.init.xavier_normal_(self.V.weight.data)\n",
    "            nn.init.xavier_normal_(self.out_layer.weight.data)\n",
    "\n",
    "    def attention(self, linear, a, N, data, edge):\n",
    "        data = linear(data).unsqueeze(0)\n",
    "        assert not torch.isnan(data).any()\n",
    "        # edge: 2*D x E\n",
    "        h = torch.cat((data[:, edge[0, :], :], data[:, edge[1, :], :]), dim=0)\n",
    "        data = data.squeeze(0)\n",
    "        # h: N x out\n",
    "        assert not torch.isnan(h).any()\n",
    "        # edge_h: 2*D x E\n",
    "        edge_h = torch.cat((h[0, :, :], h[1, :, :]), dim=1).transpose(0, 1)\n",
    "        # edge: 2*D x E\n",
    "        edge_e = torch.exp(self.leakyrelu(a.mm(edge_h).squeeze()) / np.sqrt(self.hidden_features * self.num_of_heads))\n",
    "        assert not torch.isnan(edge_e).any()\n",
    "        # edge_e: E\n",
    "        edge_e = torch.sparse_coo_tensor(edge.to(device), edge_e.to(device), torch.Size([N, N]))\n",
    "        e_rowsum = torch.sparse.mm(edge_e, torch.ones(size=(N, 1)).to(device))\n",
    "        # e_rowsum: N x 1\n",
    "        row_check = (e_rowsum == 0)\n",
    "        e_rowsum[row_check] = 1\n",
    "        zero_idx = row_check.nonzero()[:, 0]\n",
    "        edge_e = edge_e.add(\n",
    "            torch.sparse.FloatTensor(zero_idx.repeat(2, 1), torch.ones(len(zero_idx)).to(device), torch.Size([N, N])))\n",
    "        # edge_e: E\n",
    "        h_prime = torch.sparse.mm(edge_e, data)\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "        # h_prime: N x out\n",
    "        h_prime.div_(e_rowsum)\n",
    "        # h_prime: N x out\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "        return h_prime\n",
    "\n",
    "    def forward(self, edge, data=None):\n",
    "        N = self.num_of_nodes\n",
    "        if self.concat:\n",
    "            h_prime = torch.cat([self.attention(l, a, N, data, edge) for l, a in zip(self.W, self.a)], dim=1)\n",
    "        else:\n",
    "            h_prime = torch.stack([self.attention(l, a, N, data, edge) for l, a in zip(self.W, self.a)], dim=0).mean(\n",
    "                dim=0)\n",
    "        h_prime = self.dropout(h_prime)\n",
    "        if self.concat:\n",
    "            return F.elu(self.norm(h_prime))\n",
    "        else:\n",
    "            return self.V(F.relu(self.norm(h_prime)))\n",
    "\n",
    "\n",
    "class VariationalGNN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, num_of_nodes, n_heads, n_layers,\n",
    "                 dropout, alpha, variational=True, none_graph_features=0, concat=True):\n",
    "        super(VariationalGNN, self).__init__()\n",
    "        self.variational = variational\n",
    "        self.num_of_nodes = num_of_nodes + 1 - none_graph_features\n",
    "        self.embed = nn.Embedding(self.num_of_nodes, in_features, padding_idx=0)\n",
    "        self.in_att = clones(\n",
    "            GraphLayer(in_features, in_features, in_features, self.num_of_nodes,\n",
    "                       n_heads, dropout, alpha, concat=True), n_layers)\n",
    "        self.out_features = out_features\n",
    "        self.out_att = GraphLayer(in_features, in_features, out_features, self.num_of_nodes,\n",
    "                                  n_heads, dropout, alpha, concat=False)\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.parameterize = nn.Linear(out_features, out_features * 2)\n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.Linear(out_features, out_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(out_features, 1))\n",
    "        self.none_graph_features = none_graph_features\n",
    "        if none_graph_features > 0:\n",
    "            self.features_ffn = nn.Sequential(\n",
    "                nn.Linear(none_graph_features, out_features//2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout))\n",
    "            self.out_layer = nn.Sequential(\n",
    "                nn.Linear(out_features + out_features//2, out_features),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(out_features, 1))\n",
    "        for i in range(n_layers):\n",
    "            self.in_att[i].initialize()\n",
    "\n",
    "    def data_to_edges(self, data):\n",
    "        length = data.size()[0]\n",
    "        nonzero = data.nonzero()\n",
    "        if nonzero.size()[0] == 0:\n",
    "            return torch.LongTensor([[0], [0]]), torch.LongTensor([[length + 1], [length + 1]])\n",
    "        if self.training:\n",
    "            mask = torch.rand(nonzero.size()[0])\n",
    "            mask = mask > 0.05\n",
    "            nonzero = nonzero[mask]\n",
    "            if nonzero.size()[0] == 0:\n",
    "                return torch.LongTensor([[0], [0]]), torch.LongTensor([[length + 1], [length + 1]])\n",
    "        nonzero = nonzero.transpose(0, 1) + 1\n",
    "        lengths = nonzero.size()[1]\n",
    "        input_edges = torch.cat((nonzero.repeat(1, lengths),\n",
    "                                 nonzero.repeat(lengths, 1).transpose(0, 1)\n",
    "                                 .contiguous().view((1, lengths ** 2))), dim=0)\n",
    "\n",
    "        nonzero = torch.cat((nonzero, torch.LongTensor([[length + 1]]).to(device)), dim=1)\n",
    "        lengths = nonzero.size()[1]\n",
    "        output_edges = torch.cat((nonzero.repeat(1, lengths),\n",
    "                                  nonzero.repeat(lengths, 1).transpose(0, 1)\n",
    "                                  .contiguous().view((1, lengths ** 2))), dim=0)\n",
    "        return input_edges.to(device), output_edges.to(device)\n",
    "\n",
    "    def reparameterise(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = std.data.new(std.size()).normal_()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def encoder_decoder(self, data):\n",
    "        N = self.num_of_nodes\n",
    "        input_edges, output_edges = self.data_to_edges(data)\n",
    "        h_prime = self.embed(torch.arange(N).long().to(device))\n",
    "        for attn in self.in_att:\n",
    "            h_prime = attn(input_edges, h_prime)\n",
    "        if self.variational:\n",
    "            h_prime = self.parameterize(h_prime).view(-1, 2, self.out_features)\n",
    "            h_prime = self.dropout(h_prime)\n",
    "            mu = h_prime[:, 0, :]\n",
    "            logvar = h_prime[:, 1, :]\n",
    "            h_prime = self.reparameterise(mu, logvar)\n",
    "            mu = mu[data, :]\n",
    "            logvar = logvar[data, :]\n",
    "        h_prime = self.out_att(output_edges, h_prime)\n",
    "        if self.variational:\n",
    "            return h_prime[-1], 0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2)) / mu.size()[0]\n",
    "        else:\n",
    "            return h_prime[-1], torch.tensor(0.0).to(device)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Concate batches\n",
    "        batch_size = data.size()[0]\n",
    "        # In eicu data the first feature whether have be admitted before is not included in the graph\n",
    "        if self.none_graph_features == 0:\n",
    "            outputs = [self.encoder_decoder(data[i, :]) for i in range(batch_size)]\n",
    "            return self.out_layer(F.relu(torch.stack([out[0] for out in outputs]))), \\\n",
    "                   torch.sum(torch.stack([out[1] for out in outputs]))\n",
    "        else:\n",
    "            outputs = [(data[i, :self.none_graph_features],\n",
    "                        self.encoder_decoder(data[i, self.none_graph_features:])) for i in range(batch_size)]\n",
    "            return self.out_layer(F.relu(\n",
    "                torch.stack([torch.cat((self.features_ffn(torch.FloatTensor([out[0]]).to(device)), out[1][0]))\n",
    "                             for out in outputs]))), \\\n",
    "                   torch.sum(torch.stack([out[1][1] for out in outputs]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f165b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# output path of model checkpoints\n",
    "result_path = '/ml/ehr/mimic/models/notebook-run'\n",
    "# input path of processed dataset\n",
    "data_path = '/ml/ehr/mimic/out2/'\n",
    "embedding_size = 128\n",
    "# number of graph layers\n",
    "num_of_layers = 2\n",
    "# number of attention heads\n",
    "num_of_heads = 1\n",
    "# learning rate\n",
    "lr = 1e-4\n",
    "batch_size = 10\n",
    "dropout = 0.4\n",
    "# regularization \n",
    "reg = True\n",
    "# regularization\n",
    "lbd = 1.0\n",
    "in_feature = embedding_size\n",
    "out_feature = embedding_size\n",
    "n_layers = num_of_layers - 1\n",
    "n_heads = num_of_heads\n",
    "alpha = 0.1\n",
    "number_of_epochs = 1\n",
    "eval_freq = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3d4920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "def train(data, model, optim, criterion, lbd, max_clip_norm=5):\n",
    "    model.train()\n",
    "    input = data[:, :-1].to(device)\n",
    "    label = data[:, -1].float().to(device)\n",
    "    model.train()\n",
    "    optim.zero_grad()\n",
    "    logits, kld = model(input)\n",
    "    logits = logits.squeeze(-1)\n",
    "    kld = kld.sum()\n",
    "    bce = criterion(logits, label)\n",
    "    loss = bce + lbd * kld\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_clip_norm)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    return loss.item(), kld.item(), bce.item()\n",
    "\n",
    "\n",
    "def evaluate(model, data_iter, length):\n",
    "    model.eval()\n",
    "    y_pred = np.zeros(length)\n",
    "    y_true = np.zeros(length)\n",
    "    y_prob = np.zeros(length)\n",
    "    pointer = 0\n",
    "    for data in data_iter:\n",
    "        input = data[:, :-1].to(device)\n",
    "        label = data[:, -1]\n",
    "        batch_size = len(label)\n",
    "        probability, _ = model(input)\n",
    "        probability = torch.sigmoid(probability.squeeze(-1).detach())\n",
    "        predicted = probability > 0.5\n",
    "        y_true[pointer: pointer + batch_size] = label.numpy()\n",
    "        y_pred[pointer: pointer + batch_size] = predicted.cpu().numpy()\n",
    "        y_prob[pointer: pointer + batch_size] = probability.cpu().numpy()\n",
    "        pointer += batch_size\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    return auc(recall, precision), (y_pred, y_prob, y_true)\n",
    "\n",
    "\n",
    "class EHRData(Dataset):\n",
    "    def __init__(self, data, cla):\n",
    "        self.data = data\n",
    "        self.cla = cla\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cla)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.cla[idx]\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    # padding\n",
    "    data_list = []\n",
    "    for datum in data:\n",
    "        data_list.append(np.hstack((datum[0].toarray().ravel(), datum[1])))\n",
    "    return torch.from_numpy(np.array(data_list)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54467c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_x, train_y = pickle.load(open(data_path + 'train_csr.pkl', 'rb'))\n",
    "val_x, val_y = pickle.load(open(data_path + 'validation_csr.pkl', 'rb'))\n",
    "test_x, test_y = pickle.load(open(data_path + 'test_csr.pkl', 'rb'))\n",
    "train_upsampling = np.concatenate((np.arange(len(train_y)), np.repeat(np.where(train_y == 1)[0], 1)))\n",
    "train_x = train_x[train_upsampling]\n",
    "train_y = train_y[train_upsampling]\n",
    "\n",
    "# Create result root\n",
    "s = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "result_root = '%s/lr_%s-input_%s-output_%s-dropout_%s'%(result_path, lr, in_feature, out_feature, dropout)\n",
    "if not os.path.exists(result_root):\n",
    "    os.mkdir(result_root)\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "logging.basicConfig(filename='%s/train.log' % result_root, format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "logging.info(\"Time:%s\" %(s))\n",
    "\n",
    "# initialize models\n",
    "num_of_nodes = train_x.shape[1] + 1\n",
    "device_ids = range(torch.cuda.device_count())\n",
    "    \n",
    "# eICU has 1 feature on previous readmission that we didn't include in the graph\n",
    "model = VariationalGNN(in_feature, out_feature, num_of_nodes, n_heads, n_layers,\n",
    "                           dropout=dropout, alpha=alpha, variational=reg, none_graph_features=0).to(device)\n",
    "model = nn.DataParallel(model, device_ids=device_ids)\n",
    "val_loader = DataLoader(dataset=EHRData(val_x, val_y), batch_size=batch_size,\n",
    "                            collate_fn=collate_fn, num_workers=torch.cuda.device_count(), shuffle=False)\n",
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=lr, weight_decay=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# Train models\n",
    "for epoch in range(number_of_epochs):\n",
    "    print(\"Learning rate:{}\".format(optimizer.param_groups[0]['lr']))\n",
    "    ratio = Counter(train_y)\n",
    "    train_loader = DataLoader(dataset=EHRData(train_x, train_y), batch_size=batch_size,\n",
    "                                  collate_fn=collate_fn, num_workers=torch.cuda.device_count(), shuffle=True)\n",
    "    pos_weight = torch.ones(1).float().to(device) * (ratio[True] / ratio[False])\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"sum\", pos_weight=pos_weight)\n",
    "    t = tqdm(iter(train_loader), leave=False, total=len(train_loader))\n",
    "    model.train()\n",
    "    total_loss = np.zeros(3)\n",
    "    for idx, batch_data in enumerate(t):\n",
    "        loss, kld, bce = train(batch_data, model, optimizer, criterion, lbd, 5)\n",
    "        total_loss += np.array([loss, bce, kld])\n",
    "        if idx % eval_freq == 0 and idx > 0:\n",
    "            torch.save(model.state_dict(), \"{}/parameter{}_{}\".format(result_root, epoch, idx))\n",
    "            val_auprc, _ = evaluate(model, val_loader, len(val_y))\n",
    "            logging.info('epoch:%d AUPRC:%f; loss: %.4f, bce: %.4f, kld: %.4f' %\n",
    "                             (epoch + 1, val_auprc, total_loss[0]/idx, total_loss[1]/idx, total_loss[2]/idx))\n",
    "            print('epoch:%d AUPRC:%f; loss: %.4f, bce: %.4f, kld: %.4f' %\n",
    "                    (epoch + 1, val_auprc, total_loss[0]/idx, total_loss[1]/idx, total_loss[2]/idx))\n",
    "        if idx % 50 == 0 and idx > 0:\n",
    "            t.set_description('[epoch:%d] loss: %.4f, bce: %.4f, kld: %.4f' %\n",
    "                                  (epoch + 1, total_loss[0]/idx, total_loss[1]/idx, total_loss[2]/idx))\n",
    "            t.refresh()\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
